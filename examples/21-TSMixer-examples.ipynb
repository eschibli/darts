{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Mixer (TSMixer)\n",
    "This notebook walks through how to use Darts' `TSMixerModel` and benchmarks it against `TiDEModel`.\n",
    "\n",
    "TSMixer (Time-series Mixer) is an all-MLP architecture for time series forecasting. \n",
    "\n",
    "It does so by integrating historical time series data, future known inputs, and static contextual information. The architecture uses a combination of conditional feature mixing and mixer layers to process and combine these different types of data for effective forecasting.\n",
    "\n",
    "Translated to Darts, this model supports all types of covariates (past, future, and/or static).\n",
    "\n",
    "See the original paper and model description [here](https://arxiv.org/abs/2303.06053).\n",
    "\n",
    "According to the authors, the model outperforms several state-of-the-art models on multivariate forecasting tasks.\n",
    "\n",
    "Let's see how it performs against `TideModel` on the ETTh1 and ETTh2 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix python path if working locally\n",
    "from utils import fix_pythonpath_if_working_locally\n",
    "\n",
    "fix_pythonpath_if_working_locally()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from darts import concatenate\n",
    "from darts.dataprocessing.transformers.scaler import Scaler\n",
    "from darts.datasets import ETTh1Dataset, ETTh2Dataset\n",
    "from darts.metrics import mae, mse, mql\n",
    "from darts.models import TiDEModel, TSMixerModel, TSMixerResidModel\n",
    "from darts.utils.likelihood_models import QuantileRegression\n",
    "from darts.utils.callbacks import TFMProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and preparation\n",
    "We consider the ETTh1 and ETTh2 datasets which contain hourly multivariate data of an electricity transformer (load, oil temperature, ...).\n",
    "You can find more information [here](https://unit8co.github.io/darts/generated_api/darts.datasets.html#darts.datasets.ETTh1Dataset).\n",
    "\n",
    "We will add static information to each transformer time series, that identifies whether it is the `ETTh1` or `ETTh2` transformer.\n",
    "Both TSMixer and TiDE can levarage this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = []\n",
    "for idx, ds in enumerate([ETTh1Dataset, ETTh2Dataset]):\n",
    "    trafo = ds().load().astype(np.float32)\n",
    "    trafo = trafo.with_static_covariates(pd.DataFrame({\"transformer_id\": [idx]}))\n",
    "    series.append(trafo)\n",
    "series[0].pd_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, we split the data into train, validation, and test sets. The model will learn from the train set, use the validation set to determine when to stop training, and finally be evaluated on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = [], [], []\n",
    "for trafo in series:\n",
    "    train_, temp = trafo.split_after(0.6)\n",
    "    val_, test_ = temp.split_after(0.5)\n",
    "    train.append(train_)\n",
    "    val.append(val_)\n",
    "    test.append(test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the splits for the first column \"HUFL\" for each transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_col = \"HUFL\"\n",
    "for idx, (train_, val_, test_) in enumerate(zip(train, val, test)):\n",
    "    train_[show_col].plot(label=f\"train_trafo_{idx}\")\n",
    "    val_[show_col].plot(label=f\"val_trafo_{idx}\")\n",
    "    test_[show_col].plot(label=f\"test_trafo_{idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's scale the data. To avoid leaking information from the validation and test sets, we scale the data based on the properties of the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Scaler()  # default uses sklearn's MinMaxScaler\n",
    "train = scaler.fit_transform(train)\n",
    "val = scaler.transform(val)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameter Setup\n",
    "Boilerplate code is no fun, especially in the context of training multiple models to compare performance. To avoid this, we use a common configuration that can be used with any Darts `TorchForecastingModel`.\n",
    "\n",
    "A few interesting things about these parameters:\n",
    "\n",
    "- **Gradient clipping:** Mitigates exploding gradients during backpropagation by setting an upper limit on the gradient for a batch.\n",
    "- **Learning rate:** The majority of the learning done by a model is in the earlier epochs. As training goes on it is often helpful to reduce the learning rate to fine-tune the model. That being said, it can also lead to significant overfitting.\n",
    "- **Early stopping:** To avoid overfitting, we can use early stopping. It monitors a metric on the validation set and stops training once the metric is not improving anymore based on a custom condition.\n",
    "- **Likelihood and Loss Functions:** You can either make the model probabilistic with a `likelihood`, or deterministic with a `loss_fn`. In this notebook we train probabilistic models using QuantileRegression.\n",
    "- **Reversible Instance Normalization:** Use [Reversible Instance Normalization](https://openreview.net/forum?id=cGDAkQo1C0p) which in most of the cases improves model performance.\n",
    "- **Encoders:** We can encode time axis/calendar information and use them as past or future covariates using `add_encoders`. Here, we'll add cyclic encodings of the hour, day of the week, and month as future covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_params(\n",
    "    input_chunk_length: int,\n",
    "    output_chunk_length: int,\n",
    "    full_training=True,\n",
    "    past_covs=False,\n",
    "    future_covs=True,\n",
    "):\n",
    "    # early stopping: this setting stops training once the the validation\n",
    "    # loss has not decreased by more than 1e-5 for 10 epochs\n",
    "    early_stopper = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=10,\n",
    "        min_delta=1e-5,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    # PyTorch Lightning Trainer arguments (you can add any custom callback)\n",
    "    if full_training:\n",
    "        limit_train_batches = None\n",
    "        limit_val_batches = None\n",
    "        max_epochs = 200\n",
    "        batch_size = 256\n",
    "    else:\n",
    "        limit_train_batches = 20\n",
    "        limit_val_batches = 10\n",
    "        max_epochs = 40\n",
    "        batch_size = 64\n",
    "\n",
    "    # only show the training and prediction progress bars\n",
    "    progress_bar = TFMProgressBar(\n",
    "        enable_sanity_check_bar=False, enable_validation_bar=False\n",
    "    )\n",
    "    pl_trainer_kwargs = {\n",
    "        \"gradient_clip_val\": 1,\n",
    "        \"max_epochs\": max_epochs,\n",
    "        \"limit_train_batches\": limit_train_batches,\n",
    "        \"limit_val_batches\": limit_val_batches,\n",
    "        \"accelerator\": \"auto\",\n",
    "        \"callbacks\": [early_stopper, progress_bar],\n",
    "    }\n",
    "\n",
    "    # optimizer setup, uses Adam by default\n",
    "    optimizer_cls = torch.optim.Adam\n",
    "    optimizer_kwargs = {\n",
    "        \"lr\": 1e-4,\n",
    "    }\n",
    "\n",
    "    # learning rate scheduler\n",
    "    lr_scheduler_cls = torch.optim.lr_scheduler.ExponentialLR\n",
    "    lr_scheduler_kwargs = {\"gamma\": 0.999}\n",
    "\n",
    "    # for probabilistic models, we use quantile regression, and set `loss_fn` to `None`\n",
    "    likelihood = QuantileRegression()\n",
    "    loss_fn = None\n",
    "\n",
    "    add_encoders = None\n",
    "    if past_covs and future_covs:\n",
    "        add_encoders = {\n",
    "            \"cyclic\": {\n",
    "                \"future\": [\"hour\", \"dayofweek\", \"month\"],\n",
    "                \"past\": [\"hour\", \"dayofweek\", \"month\"],\n",
    "            }\n",
    "        }\n",
    "    elif past_covs:\n",
    "        add_encoders = {\n",
    "            \"cyclic\": {\n",
    "                \"past\": [\"hour\", \"dayofweek\", \"month\"],\n",
    "            }\n",
    "        }\n",
    "    elif future_covs:\n",
    "        add_encoders = {\n",
    "            \"cyclic\": {\n",
    "                \"future\": [\"hour\", \"dayofweek\", \"month\"],\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "    params = {\n",
    "        \"input_chunk_length\": input_chunk_length,  # lookback window\n",
    "        \"output_chunk_length\": output_chunk_length,  # forecast/lookahead window\n",
    "        \"use_reversible_instance_norm\": True,\n",
    "        \"optimizer_kwargs\": optimizer_kwargs,\n",
    "        \"pl_trainer_kwargs\": pl_trainer_kwargs,\n",
    "        \"lr_scheduler_cls\": lr_scheduler_cls,\n",
    "        \"lr_scheduler_kwargs\": lr_scheduler_kwargs,\n",
    "        \"likelihood\": likelihood,  # use a `likelihood` for probabilistic forecasts\n",
    "        \"loss_fn\": loss_fn,  # use a `loss_fn` for determinsitic model\n",
    "        \"save_checkpoints\": True,  # checkpoint to retrieve the best performing model state,\n",
    "        \"force_reset\": True,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"random_state\": 42}\n",
    "\n",
    "    if add_encoders:\n",
    "        params[\"add_encoders\"] = add_encoders\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model configuration\n",
    "Let's use the last week of hourly data as lookback window (`input_chunk_length`) and train a probabilistic model to predict the next 24 hours directly (`output_chunk_length`). Additionally, we tell the model to use the static information. To keep the notebook simple, we'll set `full_training=False`. To get even better performance, set `full_training=True`.\n",
    "\n",
    "Apart from that, we use our helper function to set up all the common model arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_chunk_length = 6 * 24\n",
    "output_chunk_length = 24\n",
    "use_static_covariates = True\n",
    "full_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the models\n",
    "model_tsm = TSMixerModel(\n",
    "    **create_params(\n",
    "        input_chunk_length,\n",
    "        output_chunk_length,\n",
    "        full_training=full_training,\n",
    "    ),\n",
    "    use_static_covariates=use_static_covariates,\n",
    "    model_name=\"tsm\",\n",
    ")\n",
    "\n",
    "model_tsm_basic_w_past = TSMixerModel(\n",
    "    **create_params(\n",
    "        input_chunk_length,\n",
    "        output_chunk_length,\n",
    "        full_training=full_training,\n",
    "        past_covs=True,\n",
    "        future_covs=False,\n",
    "    ),\n",
    "    use_static_covariates=use_static_covariates,\n",
    "    project_first_layer=False,\n",
    "    model_name=\"tsm_basic_w_past\",\n",
    ")\n",
    "\n",
    "model_tsm_basic = TSMixerModel(\n",
    "    **create_params(\n",
    "        input_chunk_length,\n",
    "        output_chunk_length,\n",
    "        full_training=full_training,\n",
    "        future_covs=True,\n",
    "    ),\n",
    "    use_static_covariates=use_static_covariates,\n",
    "    project_first_layer=False,\n",
    "    model_name=\"tsm_basic\",\n",
    ")\n",
    "\n",
    "model_tsm_resid = TSMixerResidModel(\n",
    "    **create_params(\n",
    "        input_chunk_length,\n",
    "        output_chunk_length,\n",
    "        full_training=full_training,\n",
    "    ),\n",
    "    hidden_size=65,\n",
    "    ff_size=129,\n",
    "    use_static_covariates=use_static_covariates,\n",
    "    model_name=\"tsm_resid\",\n",
    ")\n",
    "\n",
    "model_tide = TiDEModel(\n",
    "    **create_params(\n",
    "        input_chunk_length,\n",
    "        output_chunk_length,\n",
    "        full_training=full_training,\n",
    "    ),\n",
    "    use_static_covariates=use_static_covariates,\n",
    "    model_name=\"tide\",\n",
    ")\n",
    "models = {\n",
    "    \"TSM Residual\": model_tsm_resid,\n",
    "    \"TSM Basic\": model_tsm_basic,\n",
    "    \"TSM Basic (past only)\": model_tsm_basic_w_past,\n",
    "    \"TSM\": model_tsm,\n",
    "    \"TiDE\": model_tide,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train all of the models. When using early stopping it is important to save checkpoints. This allows us to continue past the best model configuration and then restore the optimal weights once training has been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the models and load the model from its best state/checkpoint\n",
    "for model_name, model in models.items():\n",
    "    model.fit(\n",
    "        series=train,\n",
    "        val_series=val,\n",
    "    )\n",
    "    # load from checkpoint returns a new model object, we store it in the models dict\n",
    "    models[model_name] = model.load_from_checkpoint(\n",
    "        model_name=model.model_name, best=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest the probabilistic models\n",
    "\n",
    "Let's configure the prediction. For this example, we will:\n",
    "\n",
    "- generate **historical forecasts** on the test set using the **pre-trained models**. Each forecast covers a 24 hour horizon, and the time between two consecutive forecasts is also 24 hours. This will give us **276 multivariate forecasts per transformer** to evaluate the model!\n",
    "- generate **500 stochastic samples** for each prediction point (since we have trained probabilistic models)\n",
    "- evaluate/**backtest** the probabilistic historical forecasts for some quantiles **using the Mean Quantile Loss** (`mql()`).\n",
    "\n",
    "And we'll create some helper functions to generate the forecasts, compute the backtest, and to visualize the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the probabilistic prediction\n",
    "num_samples = 500\n",
    "forecast_horizon = output_chunk_length\n",
    "\n",
    "# compute the Mean Quantile Loss over these quantiles\n",
    "evaluate_quantiles = [0.05, 0.1, 0.2, 0.5, 0.8, 0.9, 0.95]\n",
    "\n",
    "\n",
    "def historical_forecasts(model):\n",
    "    \"\"\"Generates probabilistic historical forecasts for each transformer\n",
    "    and returns the inverse transformed results.\n",
    "\n",
    "    Each forecast covers 24h (forecast_horizon). The time between two forecasts\n",
    "    (stride) is also 24 hours.\n",
    "    \"\"\"\n",
    "    hfc = model.historical_forecasts(\n",
    "        series=test,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "        stride=forecast_horizon,\n",
    "        last_points_only=False,\n",
    "        retrain=False,\n",
    "        num_samples=num_samples,\n",
    "        verbose=True,\n",
    "    )\n",
    "    return scaler.inverse_transform(hfc)\n",
    "\n",
    "\n",
    "def backtest(model, hfc, name):\n",
    "    \"\"\"Evaluates probabilistic historical forecasts using the Mean Quantile\n",
    "    Loss (MQL) over a set of quantiles.\"\"\"\n",
    "    # add metric specific kwargs\n",
    "    metric_kwargs = [{\"q\": q} for q in evaluate_quantiles]\n",
    "    metrics = [mql for _ in range(len(evaluate_quantiles))]\n",
    "    bt = model.backtest(\n",
    "        series=series,\n",
    "        historical_forecasts=hfc,\n",
    "        last_points_only=False,\n",
    "        metric=metrics,\n",
    "        metric_kwargs=metric_kwargs,\n",
    "        verbose=True,\n",
    "    )\n",
    "    bt = pd.DataFrame(\n",
    "        bt,\n",
    "        columns=[f\"q_{q}\" for q in evaluate_quantiles],\n",
    "        index=[f\"{trafo}_{name}\" for trafo in [\"ETTh1\", \"ETTh2\"]],\n",
    "    )\n",
    "    return bt\n",
    "\n",
    "\n",
    "def generate_plots(n_days, hfcs):\n",
    "    \"\"\"Plot the probabilistic forecasts for each model, transformer and transformer\n",
    "    feature against the ground truth.\"\"\"\n",
    "    # concatenate historical forecasts into contiguous time series\n",
    "    # (works because forecast_horizon=stride)\n",
    "    hfcs_plot = {}\n",
    "    for model_name, hfc_model in hfcs.items():\n",
    "        hfcs_plot[model_name] = [\n",
    "            concatenate(hfc_series[-n_days:], axis=0) for hfc_series in hfc_model\n",
    "        ]\n",
    "\n",
    "    # remember start and end points for plotting the target series\n",
    "    hfc_ = hfcs_plot[model_name][0]\n",
    "    start, end = hfc_.start_time(), hfc_.end_time()\n",
    "\n",
    "    # for each target column...\n",
    "    for col in series[0].columns:\n",
    "        fig, axes = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "        # ... and for each transformer...\n",
    "        for trafo_idx, trafo in enumerate(series):\n",
    "            trafo[col][start:end].plot(label=\"ground truth\", ax=axes[trafo_idx])\n",
    "            # ... plot the historical forecasts for each model\n",
    "            for model_name, hfc in hfcs_plot.items():\n",
    "                hfc[trafo_idx][col].plot(\n",
    "                    label=model_name + \"_q0.05-q0.95\", ax=axes[trafo_idx]\n",
    "                )\n",
    "            axes[trafo_idx].set_title(f\"ETTh{trafo_idx + 1}: {col}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we're ready to evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bts = {}\n",
    "hfcs = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"Generating historical forecasts..\")\n",
    "    hfcs[model_name] = historical_forecasts(models[model_name])\n",
    "\n",
    "    print(\"Evaluating historical forecasts..\")\n",
    "    bts[model_name] = backtest(models[model_name], hfcs[model_name], model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how they performed.\n",
    "\n",
    "> **Note:** These results are likely to improve/change when setting `full_training=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_df = pd.concat(bts.values(), axis=0).sort_index()\n",
    "bt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backtest gives us the Mean Quantile Loss for the selected quantiles over all transformer features per transformer and model. The lower the value, the better. The `q_0.5` is identical to the Mean Absolute Error (MAE) between the median prediction and the ground truth.\n",
    "\n",
    "Both models seem to have performed comparably well. And how does it look on average over all quantiles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the results are also very similar. It seems that TSMixer performed better for ETTh1, and TiDEModel for ETTh2.\n",
    "\n",
    "And last but not least, let's have look at the predictions for the last `n_days=3` days in the test set.\n",
    "\n",
    "> Note: The prediction intervals are expected to get narrower when `full_training=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plots(n_days=3, hfcs=hfcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "In this case, `TSMixer` and `TiDEModel` both perform similarly well. Keep in mind that we performed only partial training on the data, and that we used the default model parameters without any hyperparameter tuning. \n",
    "\n",
    "Here are some ways to further improve the performance:\n",
    "\n",
    "- set `full_training=True`\n",
    "- perform hyperparameter tuning\n",
    "- add more covariates (we have only added cyclic encodings of calendar information)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
